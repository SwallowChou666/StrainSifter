Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Unlimited resources: mem, time
Job counts:
	count	jobs
	1	filter_samples
	1

rule filter_samples:
    input: coverage/S82.cvg
    output: passed_samples/S82.bam
    jobid: 0
    wildcards: sample=S82
    resources: mem=1, time=1

Error in rule filter_samples:
    jobid: 0
    output: passed_samples/S82.bam

RuleException:
CalledProcessError in line 80 of /srv/gsfs0/projects/bhatt/fiona/bacteremia/strainsifter/Snakefile:
Command ' set -euo pipefail;  if ($(cat coverage/S82.cvg | awk '{if ($2 >= 5 && $3 >= 0.5) print "true"; else print "false"}') == true);then ln -s ~/fiona/bacteremia/10.call_variants_v2/filtered_bam/S82.filtered.bam passed_samples/S82.bam && touch -h passed_samples/S82.bam;else touch passed_samples/S82.bam; fiif ($(cat coverage/S82.cvg | awk '{if ($2 >= 5 && $3 >= 0.5) print "true"; else print "false"}') == true); then touch passed_samples/S82.bam; fi ' returned non-zero exit status 1.
  File "/srv/gsfs0/projects/bhatt/fiona/bacteremia/strainsifter/Snakefile", line 80, in __rule_filter_samples
  File "/home/tamburin/tools/miniconda3/lib/python3.6/concurrent/futures/thread.py", line 55, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /srv/gsfs0/projects/bhatt/fiona/bacteremia/strainsifter/.snakemake/log/2018-04-28T145603.557310.snakemake.log
